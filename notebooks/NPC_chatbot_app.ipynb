{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#NPC Chatbot App with ChatGPT, LangChain and Streamlit"
      ],
      "metadata": {
        "id": "HSD5WnWeUhpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Dependencies"
      ],
      "metadata": {
        "id": "9QQK9X0HUdQn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqN2lxJAcVgq"
      },
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "!pip install langchain==0.1.12\n",
        "!pip install langchain-openai==0.0.8\n",
        "!pip install langchain-community==0.0.29\n",
        "!pip install streamlit==1.32.2\n",
        "!pip install chromadb==0.4.24\n",
        "!pip install pyngrok==7.1.5\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load OpenAI API Credentials"
      ],
      "metadata": {
        "id": "ZAQUrLx2UmRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OpenAI API Credentials\n",
        "from getpass import getpass\n",
        "\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')\n"
      ],
      "metadata": {
        "id": "taXEV0M0cbRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set Environment Variable"
      ],
      "metadata": {
        "id": "opiX93NcUpHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ],
      "metadata": {
        "id": "pSJUybmXUwe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Write App Code Header"
      ],
      "metadata": {
        "id": "h1CnU8_1U0yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "from langchain.chains import LLMChain\n",
        "from operator import itemgetter\n",
        "\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Customize initial app landing page\n",
        "st.set_page_config(page_title=\"NPC Simulation Chatbot\", page_icon=\"ðŸŽ­\")\n",
        "st.title(\"NPC Simulation Chatbot ðŸŽ­\")\n",
        "st.sidebar.header(\"NPC Character Document Upload\")\n",
        "\n",
        "@st.cache_resource(ttl=\"1h\")\n",
        "def configure_npc_agent(uploaded_file):\n",
        "    if uploaded_file is not None:\n",
        "        text_content = uploaded_file.read().decode(\"utf-8\")\n",
        "\n",
        "        # --- Normalize line endings to standard Unix-style (\\n) ---\n",
        "        text_content = text_content.replace('\\\\r\\\\n', '\\\\n').replace('\\\\r', '\\\\n')\n",
        "\n",
        "        # --- SECTION PARSING: Line-based parsing ---\n",
        "        lines = text_content.strip().split('\\n') # Split into lines\n",
        "        sections_dict = {}\n",
        "        current_section_name = None\n",
        "        current_section_content = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\"---\") and line.endswith(\"---\"):\n",
        "                if current_section_name:\n",
        "                    sections_dict[current_section_name] = \"\\n\".join(current_section_content).strip()\n",
        "                    current_section_content = [] # Reset content for new section\n",
        "                current_section_name = line.strip('-').strip() # Extract section name\n",
        "            elif current_section_name:\n",
        "                current_section_content.append(line) # Append line to current section\n",
        "\n",
        "        if current_section_name and current_section_content: # Capture last section\n",
        "            sections_dict[current_section_name] = \"\\n\".join(current_section_content).strip()\n",
        "\n",
        "\n",
        "        character_description = sections_dict.get(\"Character Description\", \"\")\n",
        "        backstory_world_info = sections_dict.get(\"Backstory and World Info\", \"\")\n",
        "        quests_text = sections_dict.get(\"Quests\", \"\")\n",
        "\n",
        "        quests = [q.strip() for q in quests_text.strip().split(\"- \") if q.strip()]\n",
        "\n",
        "        return character_description, backstory_world_info, quests\n",
        "\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "# Manages live updates to a Streamlit app's display by appending new text tokens\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container, initial_text=\"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "# UI element to upload text document\n",
        "uploaded_file = st.sidebar.file_uploader(\n",
        "    label=\"Upload NPC Character Text Document\", type=[\"txt\"]\n",
        ")\n",
        "\n",
        "if uploaded_file is None:\n",
        "    st.info(\"Please upload your NPC Character Text Document to continue.\")\n",
        "    st.stop()\n",
        "\n",
        "# Configure NPC agent\n",
        "character_description, backstory_world_info, quests = configure_npc_agent(uploaded_file)\n",
        "\n",
        "if character_description is None or backstory_world_info is None or quests is None:\n",
        "    st.error(\"Error loading character document. Please check the format.\")\n",
        "    st.stop()\n",
        "\n",
        "# Split backstory and world info into chunks for RAG\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_overlap=200,\n",
        "    chunk_size=2000,\n",
        "    length_function=len,\n",
        ")\n",
        "doc_chunks = text_splitter.create_documents([backstory_world_info])\n",
        "\n",
        "# Create embeddings and vectorstore for backstory/world info\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(doc_chunks, embeddings_model)\n",
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "\n",
        "# Load LLM\n",
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.7, streaming=True)\n",
        "\n",
        "# System prompt for NPC character\n",
        "system_prompt_template = SystemMessagePromptTemplate.from_template(character_description)\n",
        "\n",
        "# RAG Prompt for retrieving NPC/World info\n",
        "rag_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    system_prompt_template,\n",
        "    HumanMessagePromptTemplate.from_template(\"\"\"Use the following context to answer the user question about yourself or the world.\n",
        "                                              Context: {context}\n",
        "                                              Question: {question}\n",
        "                                              If you cannot answer from the context, just say you do not know.\n",
        "                                              Answer in character as described in the system prompt.\"\"\")\n",
        "])\n",
        "\n",
        "\n",
        "# General conversation prompt (no RAG)\n",
        "conversation_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    system_prompt_template,\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "])\n",
        "\n",
        "\n",
        "# Quest related prompts\n",
        "quest_suggestion_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    system_prompt_template,\n",
        "    HumanMessagePromptTemplate.from_template(\"Suggest a quest for the user, keeping in character. If no quests are appropriate, just say 'No quests available right now.'\")\n",
        "])\n",
        "quest_suggestion_chain = quest_suggestion_prompt_template | chatgpt\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever | format_docs,\n",
        "        \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | rag_prompt_template\n",
        "    | chatgpt\n",
        ")\n",
        "conversation_chain = conversation_prompt_template | chatgpt\n",
        "\n",
        "\n",
        "# --- Question Type Classifier Chain for NPC Chatbot ---\n",
        "npc_question_type_prompt_template = \"\"\"\n",
        "Determine the type of question being asked to the NPC. Choose from the following categories: quest_request, info_request, or general_conversation.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Respond with 'quest_request' if the question is clearly asking for a quest, or about quests in general (e.g., \"give me a quest\", \"offer a quest\", \"do you have quests?\", \"what quests are available?\", \"i want a quest\", \"tell me about quests\", \"any tasks for me?\").\n",
        "Respond with 'info_request' if the question is asking for information about the NPC or their world, requiring context from their background (e.g., \"tell me about yourself\", \"who are you?\", \"where are we?\", \"what is this place?\", \"tell me about the world\").\n",
        "Respond with 'general_conversation' if the question is just a general greeting, statement, or question that doesn't clearly fall into 'quest_request' or 'info_request' (e.g., \"hello\", \"how are you?\", \"what do you do?\", \"nice weather today\").\n",
        "\n",
        "Just answer 'quest_request', 'info_request', or 'general_conversation'.\n",
        "\"\"\"\n",
        "npc_question_type_prompt = ChatPromptTemplate.from_template(npc_question_type_prompt_template)\n",
        "npc_question_type_chain = npc_question_type_prompt | chatgpt\n",
        "# --- End Question Type Classifier Chain ---\n",
        "\n",
        "\n",
        "# Conversation Handling\n",
        "streamlit_msg_history = StreamlitChatMessageHistory(key=\"npc_chat_messages\")\n",
        "\n",
        "if len(streamlit_msg_history.messages) == 0:\n",
        "    streamlit_msg_history.add_ai_message(\"Greetings! How can I help you today?\")\n",
        "\n",
        "for msg in streamlit_msg_history.messages:\n",
        "    st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "if user_prompt := st.chat_input():\n",
        "    st.chat_message(\"human\").write(user_prompt)\n",
        "\n",
        "    # --- Classify question type ---\n",
        "    question_type_response = npc_question_type_chain.invoke({\"question\": user_prompt})\n",
        "    question_type = question_type_response.content.strip()\n",
        "    st.write(f\"--- **DEBUGGING: Question Type:** --- {question_type}\") # Debugging question type\n",
        "\n",
        "    if question_type == \"quest_request\":\n",
        "        # --- DEBUGGING: Available quests on quest_request ---\n",
        "        st.write(f\"--- **DEBUGGING: AVAILABLE QUESTS ON USER QUEST REQUEST** ---\")\n",
        "        st.write(f\"Quests list at start of quest handling: {quests}\")\n",
        "        st.write(f\"--- **END DEBUGGING: AVAILABLE QUESTS** ---\")\n",
        "        # --- END DEBUGGING ---\n",
        "\n",
        "        if quests:\n",
        "            quest_to_offer = quests.pop(0) # Get the first quest and remove it from the list\n",
        "\n",
        "            # --- Direct Quest Output - Bypassing LLM for Quest Text ---\n",
        "            with st.chat_message(\"ai\"):\n",
        "                # Fixed, in-character intro (outside LLM)\n",
        "                quest_intro = \"Right then, listen up! Here's your task:\"  # Customize this intro\n",
        "\n",
        "                # Directly output the quest text using st.markdown for formatting\n",
        "                quest_message = f\"{quest_intro}\\n\\n**{quest_to_offer}**\"\n",
        "                st.markdown(quest_message)\n",
        "\n",
        "            streamlit_msg_history.add_user_message(user_prompt)\n",
        "            streamlit_msg_history.add_ai_message(quest_message)\n",
        "            # --- End Direct Quest Output ---\n",
        "\n",
        "        else:\n",
        "             with st.chat_message(\"ai\"):\n",
        "                stream_handler = StreamHandler(st.empty())\n",
        "                config = {\"callbacks\": [stream_handler]}\n",
        "                response = quest_suggestion_chain.invoke({}, config)\n",
        "             streamlit_msg_history.add_user_message(user_prompt)\n",
        "             streamlit_msg_history.add_ai_message(response.content)\n",
        "\n",
        "\n",
        "    elif question_type == \"info_request\":\n",
        "        # RAG for NPC/World info\n",
        "        with st.chat_message(\"ai\"):\n",
        "            stream_handler = StreamHandler(st.empty())\n",
        "            config = {\"callbacks\": [stream_handler]}\n",
        "            response = rag_chain.invoke({\"question\": user_prompt}, config)\n",
        "        streamlit_msg_history.add_user_message(user_prompt)\n",
        "        streamlit_msg_history.add_ai_message(response.content)\n",
        "\n",
        "    elif question_type == \"general_conversation\":\n",
        "        # General conversation\n",
        "        with st.chat_message(\"ai\"):\n",
        "            stream_handler = StreamHandler(st.empty())\n",
        "            config = {\"callbacks\": [stream_handler]}\n",
        "            response = conversation_chain.invoke({\"question\": user_prompt}, config)\n",
        "        streamlit_msg_history.add_user_message(user_prompt)\n",
        "        streamlit_msg_history.add_ai_message(response.content)\n",
        "\n",
        "    else: # Fallback, should ideally not reach here if classification is comprehensive\n",
        "        with st.chat_message(\"ai\"):\n",
        "            stream_handler = StreamHandler(st.empty())\n",
        "            config = {\"callbacks\": [stream_handler]}\n",
        "            response = conversation_chain.invoke({\"question\": user_prompt}, config)\n",
        "        streamlit_msg_history.add_user_message(user_prompt)\n",
        "        streamlit_msg_history.add_ai_message(response.content)"
      ],
      "metadata": {
        "id": "Bex1qUURomtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Starting the Streamlit App"
      ],
      "metadata": {
        "id": "GIs0U2lRU47H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting the Streamlit App\n",
        "!streamlit run app.py --server.port=8989 &>/./logs.txt &"
      ],
      "metadata": {
        "id": "RPRHFv4AcgRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting Up ngrok Tunnel"
      ],
      "metadata": {
        "id": "92ThtCCDU7Cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting Up ngrok Tunnel\n",
        "from getpass import getpass\n",
        "\n",
        "ngrok_auth_token = getpass('Enter ngrok API Key: ')"
      ],
      "metadata": {
        "id": "1yUedUx5cjkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import yaml\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Authenticate ngrok with the token read from the file\n",
        "!ngrok config add-authtoken {ngrok_auth_token}\n",
        "\n",
        "# Open an HTTPS tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "id": "pYvP6SSfcnGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}